# ğŸ¤– Local RAG Assistant

> **Assistente de Chat com IA Local usando RAG (Retrieval-Augmented Generation)**
> *Converse com seus PDFs sem enviar dados para a nuvem.*

![Status](https://img.shields.io/badge/Status-Em_Desenvolvimento-yellow)
![Python](https://img.shields.io/badge/Backend-FastAPI-blue)
![Angular](https://img.shields.io/badge/Frontend-Angular_17+-red)
![Docker](https://img.shields.io/badge/Infra-Docker-2496ED)
![AI](https://img.shields.io/badge/AI-Ollama_Local-orange)

## ğŸ“– Sobre o Projeto

O **Local RAG Assistant** Ã© uma aplicaÃ§Ã£o full-stack que permite aos usuÃ¡rios fazer upload de documentos PDF e conversar com eles usando InteligÃªncia Artificial. 

Diferente de soluÃ§Ãµes baseadas em nuvem (como ChatGPT ou Claude), este projeto roda **100% localmente** utilizando o **Ollama**, garantindo total privacidade dos dados. Ele utiliza a tÃ©cnica de **RAG (Retrieval-Augmented Generation)** para fornecer respostas precisas baseadas no contexto dos documentos enviados.

### ğŸš€ Principais Funcionalidades

*   **Privacidade Total**: Nenhum dado sai da sua mÃ¡quina.
*   **IngestÃ£o de Documentos**: Upload e processamento de PDFs com *chunking* inteligente.
*   **Busca SemÃ¢ntica**: Utiliza **ChromaDB** para armazenar e recuperar vetores de contexto.
*   **Chat Interativo**: Interface moderna em **Angular** com suporte a respostas em tempo real (streaming).
*   **Arquitetura Modular**: Backend desacoplado (FastAPI) e Frontend reativo (Angular Signals).
*   **ContainerizaÃ§Ã£o**: Setup completo via **Docker Compose**.

---

## ğŸ› ï¸ Tecnologias Utilizadas

### Backend
*   **Python 3.11+**
*   **FastAPI**: API REST de alta performance.
*   **LangChain**: Framework para orquestraÃ§Ã£o de LLMs.
*   **ChromaDB**: Banco de dados vetorial para busca semÃ¢ntica.
*   **Ollama**: Executor local de modelos de linguagem (LLMs).
*   **Pytest**: Testes unitÃ¡rios e de integraÃ§Ã£o.

### Frontend
*   **Angular (v17+)**: Framework SPA moderno.
*   **Signals**: Gerenciamento de estado reativo.
*   **Tailwind CSS / Custom CSS**: EstilizaÃ§Ã£o.

### Infraestrutura
*   **Docker & Docker Compose**: OrquestraÃ§Ã£o de containers.
*   **NVIDIA Container Toolkit**: Suporte a aceleraÃ§Ã£o por GPU (opcional).

---

## âš™ï¸ PrÃ©-requisitos

Antes de comeÃ§ar, certifique-se de ter instalado:
*   [Docker](https://www.docker.com/) e [Docker Compose](https://docs.docker.com/compose/install/).
*   *(Opcional)* Drivers NVIDIA e NVIDIA Container Toolkit para melhor performance do modelo.

---

## ğŸš€ Como Executar

### 1. Clone o RepositÃ³rio
```bash
git clone https://github.com/seu-usuario/local-rag-assistant.git
cd local-rag-assistant
```

### 2. ConfiguraÃ§Ã£o de Ambiente
Crie um arquivo `.env` na raiz do projeto (ou renomeie um exemplo, se houver):

```env
# .env
OLLAMA_BASE_URL=http://ollama:11434
CHROMA_DB_DIR=/chroma_db
DATA_PDFS_DIR=/data/files
BACKEND_PORT=8000
FRONTEND_PORT=8080
```

### 3. Inicie os Containers
Execute o comando abaixo para construir e subir os serviÃ§os:

```bash
docker-compose up -d --build
```

### 4. Inicialize os Modelos de IA
O projeto inclui um script para baixar automaticamente os modelos necessÃ¡rios (`qwen2:0.5b` para chat e `all-minilm` para embeddings) dentro do container do Ollama:

```bash
chmod +x scripts/init_models.sh
./scripts/init_models.sh
```
> *Nota: O download pode levar alguns minutos dependendo da sua conexÃ£o.*

### 5. Acessar a AplicaÃ§Ã£o
*   **Frontend**: [http://localhost:8080](http://localhost:8080)
*   **API Docs (Swagger)**: [http://localhost:8000/docs](http://localhost:8000/docs)

---

## ğŸ§ª Testes

O projeto segue boas prÃ¡ticas de desenvolvimento com testes automatizados.

### Backend (Pytest)
Os testes do backend utilizam *mocks* para simular o LLM, permitindo execuÃ§Ã£o rÃ¡pida sem carregar os modelos pesados.

```bash
# Executar testes dentro do container
docker exec rag_backend pytest -v
```

### Frontend (Karma/Jasmine)
Para rodar os testes unitÃ¡rios dos componentes Angular, Ã© recomendado executar localmente, pois o container Docker de produÃ§Ã£o (Nginx) nÃ£o possui as ferramentas de desenvolvimento.

```bash
# Na pasta frontend:
cd frontend
npm install
ng test
```

---

## ğŸ“‚ Estrutura do Projeto

```
local-rag-assistant/
â”œâ”€â”€ backend/                # API Python (FastAPI)
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py         # Entrypoint e Endpoints
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ tests/              # Testes automatizados (Pytest)
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ frontend/               # AplicaÃ§Ã£o Angular
â”‚   â”œâ”€â”€ src/app/
â”‚   â”‚   â”œâ”€â”€ components/     # Componentes (Chat, FileUpload)
â”‚   â”‚   â”œâ”€â”€ services/       # ComunicaÃ§Ã£o com API
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ data/                   # PersistÃªncia de dados (PDFs, ChromaDB)
â”œâ”€â”€ scripts/                # Scripts utilitÃ¡rios (init_models.sh)
â””â”€â”€ docker-compose.yml      # OrquestraÃ§Ã£o dos serviÃ§os
```

---

## ğŸ”® Melhorias Futuras

*   [ ] Adicionar suporte a mÃºltiplos formatos de arquivo (.docx, .txt).
*   [ ] Implementar histÃ³rico de conversas persistente.
*   [ ] Melhorar a interface de feedback de upload.
*   [ ] Adicionar autenticaÃ§Ã£o de usuÃ¡rio.

---

## ğŸ“ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Sinta-se Ã  vontade para contribuir!

---
Desenvolvido por **[Seu Nome]** ğŸš€
[LinkedIn](https://linkedin.com/in/seu-linkedin) | [GitHub](https://github.com/seu-github)
